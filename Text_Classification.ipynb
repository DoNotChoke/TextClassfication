{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tuann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tuann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() missing 1 required positional argument: 'filepath_or_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2cls_spam_text_cls.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: read_csv() missing 1 required positional argument: 'filepath_or_buffer'"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(dtype='2cls_spam_text_cls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= df['Message'].values.tolist()\n",
    "labels= df['Category'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def punctuation_removal(text):\n",
    "    translator= str.maketrans('','', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words= nltk.corpus.stopwords.words('english')\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def stemming(tokens):\n",
    "    stemmer= nltk.PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text= lower_case(text)\n",
    "    text= punctuation_removal(text)\n",
    "    tokens= tokenize(text)\n",
    "    tokens= remove_stopwords(tokens)\n",
    "    tokens= stemming(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [preprocess_text(message) for message in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(messages):\n",
    "    dictionary=[]\n",
    "    for tokens in messages:\n",
    "        for token in tokens:\n",
    "            if token not in dict:\n",
    "                dictionary.append(token)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary= create_dictionary(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(tokens, dictionary):\n",
    "    features= np.zeros(len(dictionary))\n",
    "    for token in tokens:\n",
    "        features[dictionary.index(token)] += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array([create_feature(tokens,dictionary) for tokens in messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:['ham' 'spam']\n",
      "Encoded labels:[0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Tien xy ly du lieu nhan\n",
    "le = LabelEncoder()\n",
    "y= le.fit_transform(labels)\n",
    "print(f'Classes:{le.classes_}')\n",
    "print(f'Encoded labels:{y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE= 0.2\n",
    "TEST_SIZE=0.125\n",
    "SEED= 0\n",
    "x_train, x_val,y_train, y_val= train_test_split(X, y,\n",
    "                                                test_size= VAL_SIZE,\n",
    "                                                shuffle= True,\n",
    "                                                random_state= SEED)\n",
    "x_train, x_test, y_train, y_test= train_test_split(x_train,y_train,\n",
    "                                                   test_size=TEST_SIZE,\n",
    "                                                   shuffle=True,\n",
    "                                                   random_state= SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "model = GaussianNB()\n",
    "print('Start training...')\n",
    "model = model.fit(x_train, y_train)\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 0.8816143497757848\n",
      "Test accuracy: 0.8602150537634409\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "y_val_pred= model.predict(x_val)\n",
    "y_test_pred= model.predict(x_test)\n",
    "val_accuracy= accuracy_score(y_val, y_val_pred)\n",
    "test_accuracy= accuracy_score(y_test, y_test_pred)\n",
    "print(f'Val accuracy: {val_accuracy}')\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply\n",
    "def predict(text, model, dictionary):\n",
    "    proccesed_text= preprocess_text(text)\n",
    "    features= create_feature(proccesed_text, dictionary)\n",
    "    features= np.array(features).reshape(1, -1)\n",
    "    prediction= model.predict(features)\n",
    "    prediction_label= le.inverse_transform(prediction)[0]\n",
    "    return prediction_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ham\n"
     ]
    }
   ],
   "source": [
    "test_input = 'I am actually thinking a way of doing something useful'\n",
    "prediction_cls = predict(test_input, model, dictionary)\n",
    "print(f'Prediction: {prediction_cls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
